import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SparkSession
import scala.util.control.Breaks._


class Node(var idx:Any,
           var neighbour:List[(Any,Double)],
           var E:Double,
           var I:Double=0,
           var chosen:Boolean=false,
           var partition:Boolean=true) extends Serializable {
  var __idx:Any=idx
  var __neighbour:List[(Any, Double)]=neighbour
  var __E:Double=E
  var __I:Double=I
  var __chosen:Boolean=chosen
  var __partition:Boolean=partition//表示这个点是否在第一个图里面

  def set_E(E:Double):Node={
    this.__E = E
    return this
  }
  def set_I(I:Double):Node={
    this.__I = I
    return this
  }
  def set_chosen(chosen:Boolean):Node={
    this.__chosen = chosen
    return this
  }
  def set_partition(partition:Boolean):Node={
    this.__partition = partition
    return this
  }
  def get_idx():Any=this.__idx
  def get_neighbour():List[(Any,Double)]=this.__neighbour
  def get_E():Double=this.__E
  def get_I():Double= this.__I
  def get_chosen():Boolean= this.__chosen
  def get_partition():Boolean= this.__partition

  def weight(otherNode:Node): Double ={
    val weight = this.get_neighbour().filter(x=>x._1==otherNode.get_idx())
    if(weight.length==0) return 0.0
    return weight(0)._2
  }

  def swapGain(otherNode:Node):Double=
    this.get_E()-this.get_I()+otherNode.get_E()-
      otherNode.get_I()-2*this.weight(otherNode)

  def print(): Unit ={
    println(this.__idx+" E="+this.__E+" I="+this.__I
      +" partition="+this.__partition+" is chosen="+this.__chosen)
  }
}

object KernighanLin {

  def nodeUpdate(node:Node,
                 swap_node_a:Node,
                 swap_node_b:Node):Node={
    if(swap_node_a.get_partition==swap_node_b.get_partition){
      println("输入有错误")
      return node
    }

    val is_node_a = node.get_idx==swap_node_a.get_idx
    val is_node_b = node.get_idx==swap_node_b.get_idx
    val in_a_graph = node.get_partition==swap_node_a.get_partition

    val E_a = swap_node_a.get_E
    val I_a = swap_node_a.get_I
    val E_b = swap_node_b.get_E
    val I_b = swap_node_b.get_I

    val weight_ab = swap_node_a.weight(swap_node_b)
    if(is_node_a)
      return node.set_E(I_a+weight_ab).set_I(E_a-weight_ab).
        set_chosen(true).set_partition(!swap_node_a.get_partition)
    if(is_node_b)
      return node.set_E(I_b+weight_ab).set_I(E_b-weight_ab).
        set_chosen(true).set_partition(!swap_node_b.get_partition)

    val weight_a = node.weight(swap_node_a)
    val weight_b = node.weight(swap_node_b)

    if(weight_a==0.0&&weight_b==0.0) return node

    if(in_a_graph)
      return node.set_I(
        node.get_I - weight_a + weight_b
      ).set_E(
        //这些点的E增加了与a连接的权重
        node.get_E + weight_a - weight_b
      )
    else
      return node.set_I(
        node.get_I + weight_a - weight_b
      ).set_E(
        //这些点的E增加了与a连接的权重
        node.get_E - weight_a + weight_b
      )


  }
  def graphUpdate(nodeRDD:RDD[Node],
                  swap_node_a:Node,
                  swap_node_b:Node):RDD[Node]={

    //入口参数检查

    if((swap_node_a==null)||(swap_node_b==null)||
      (swap_node_a.get_partition==swap_node_b.get_partition)||
      (swap_node_a.get_chosen==true)||(swap_node_b.get_chosen==true)){
      println("输入交换的点不合法")
      return nodeRDD
    }


    val nodeRDD_update = nodeRDD.map(
      x=>nodeUpdate(x, swap_node_a, swap_node_b)
    )

    return nodeRDD_update
  }
  def graphPartitionEvaluation(nodeRDD:RDD[Node]):Double={
    //计算图内聚和图外连
    /**
      * 把点map为E和，计算E和I的总和，
      * 然后累计起来，因为每个点都有两次相连，所以除以2*/
    val inner_external_Weight = nodeRDD.map(
      x=>(x.get_I(), x.get_E())
    ).reduce(
      (x,y)=>(x._1+y._1, x._2+y._2)
    )

    return (inner_external_Weight._1-inner_external_Weight._2)/2
  }

  def main(args: Array[String]): Unit = {

    val sparkSession = SparkSession
      .builder()
      .appName("Spark SQL basic example")
      .master("local")
      .config("spark.some.config.option", "some-value")
      .getOrCreate()
    // For implicit conversions like converting RDDs to DataFrames
    import sparkSession.implicits._
    val df = sparkSession.read.csv("test2.csv")


    /**
      * 1、首先组织一下数据，原来是两两连接的点，现在是(点:key, tuple(点，weight):value)
      *    也就是把连接点和连接的权重组织成一个tuple
      * 2、聚合相同的点，得到邻近点的集合，注意，临近点是一个tuple(点，weight)
      *    并且转化为List
      * 3、计算每个点跟两个子图连接的权重，定义了一个函数计算
      *    思路是filter出那些在子图里面的点的集合，map一下只保留权重，进而sum
      * 4、可以通过map，生成每一个点的类，partition表示这个点在第一个graph与否
      * */


    val df_rdd = df.rdd
//    val split_rdd = df_rdd.map(x=>x(0)).randomSplit(Array(0.5,0.5),1996)

    //    var graph_1  = Array[Any]("1", "2")
    //    var graph_2  = Array[Any]("3", "4")
    var graph_1  = Array[Any]("1", "2", "3")
    var graph_2  = Array[Any]("4", "5", "6")
//    var graph_1 = split_rdd(0).collect()
//    var graph_2 = split_rdd(1).collect()
//    pr

    var df_data = df_rdd.map(x=>(x(0), x(1)))
    df_data = df_data.union(df_data.map(x=>(x._2, x._1)))

    val data = df_data.map(
      x=>(x._1, (x._2, 1.0)
      ))
    data.foreach(println)



    var node_rdd = data.combineByKey(
      (x:(Any, Double))=>
        (
          List(x),
          //计算当前连接点跟graph1的连接权重
          if(graph_1.contains(x._1)) x._2 else 0.0,
          //计算当前连接点跟graph2的连接权重
          if(graph_2.contains(x._1)) x._2 else 0.0
        ),//相当于对value做了一次变换，方便计算

      (combineValue:(List[(Any,Double)], Double, Double), x:(Any, Double))=>
        (
          combineValue._1:+x,
          //合并：累计跟graph1的连接权重+同一个分区不同key对连接点跟graph1的权重
          combineValue._2+(if(graph_1.contains(x._1)) x._2 else 0.0),
          //合并：累计跟graph2的连接权重+同一个分区不同key对连接点跟graph2的权重
          combineValue._3+(if(graph_2.contains(x._1)) x._2 else 0.0)
        ),//聚合一个分区内部不同key的值，value对应领域
      //(if(partition_1.contains(x._1)) {println("haha");x._2} else 0.0
      (combineValue1:(List[(Any,Double)], Double, Double),
       combineValue2:(List[(Any,Double)], Double, Double))=>
        (
          combineValue1._1:::combineValue2._1,
          //合并不同分区的累计权重
          combineValue1._2+combineValue2._2,
          combineValue1._3+combineValue2._3
        )
    ).map(
      x=>(
        x._1, x._2._1,x._2._2,x._2._3,graph_1.contains(x._1)
      )
    ).map(
      x=>(
        x._1, x._2,
        if(x._5) x._4 else x._3,//在第一个图中，E是图2的连接，I是图1的连接
        if(x._5) x._3 else x._4,//在第一个图中，I是图2的连接，E是图1的连接
        x._5,
      )
    ).map(
      x=>new Node(x._1, x._2, x._3, x._4, false, x._5)
    )

    node_rdd.foreach(x=>x.print())

    //calculate example
    var gain_max = 0.0
    var swap_node_a:Node=null
    var swap_node_b:Node=null
    var chosenNum = 0

    var count:Int = 0
    var evalList:List[Double] = List()


    breakable{
      do{

        println("开始第%d轮次"(count))
        //        val node_rdd_ = node_rdd
        node_rdd.foreach(x=>x.print())


        swap_node_a = null
        swap_node_b = null
        gain_max = 0.0

        //不在一个子图里面
        def notInSameGraph(node1:Node, node2:Node):Boolean =
          node1.get_partition()!=node2.get_partition()
        //两个点都没有被选择过
        def unChosen(node1:Node, node2:Node):Boolean =
          (!node1.get_chosen())&&(!node2.get_chosen())
        //无向图去重
        def distinct(node1:Node, node2:Node):Boolean =
          node1.get_idx().toString<node2.get_idx().toString

        val node_gain = node_rdd.cartesian(node_rdd).filter(
          x=>
            distinct(x._1,x._2)&&
              notInSameGraph(x._1, x._2)&&
              unChosen(x._1, x._2)
        ).map(x=>{
          (x, x._1.swapGain(x._2))
        })
        //理论上来说不需要判断node_gain是否为空，如果没有满足条件的点，上一个循环以经退出
        //判断是否为空需要action，消耗较大，可以判断partitions参数是否为空，空RDD没有分区
        val max_gain_item = node_gain.reduce((x,y)=>{
          if(x._2>=y._2) x else y
        })
        if(max_gain_item._2>0){
          println("positive gain!!! find "+max_gain_item._1._1.get_idx
            +" and "+max_gain_item._1._2.get_idx+" with gain "+max_gain_item._2)
          swap_node_a = max_gain_item._1._1
          swap_node_b = max_gain_item._1._2
          gain_max = max_gain_item._2
        }
        else{
          println("negative gain!!! find "+
            max_gain_item._1._1.get_idx+" and "+
            max_gain_item._1._2.get_idx+" with gain "+max_gain_item._2)
          println("game over!!")
          println("没有可以交换的点，可以离开了")
          break()
        }

        //已经保证了交换的两个点不为空
//        if(swap_node_a==null || swap_node_b==null) {
//
//        }
        //swap
        println("swap two node : "+swap_node_a.get_idx+"and"+swap_node_b.get_idx+" wirh gain "+gain_max)
        chosenNum+=1

        node_rdd = graphUpdate(node_rdd, swap_node_a, swap_node_b)

        println("第%d轮结束"(count))
        count+=1
        node_rdd.foreach(x=>x.print())


        graph_1 = node_rdd.filter(x=>x.get_partition).map(x=>x.get_idx()).collect()
        graph_2 = node_rdd.filter(x=>x.get_partition==false).map(x=>x.get_idx()).collect()

        println("图1的点包括：")
        graph_1.foreach(x=>print(x+" "))
        println()
        println("图2的点包括：")
        graph_2.foreach(x=>print(x+" "))
        print("现在的K-L值是="+(graphPartitionEvaluation(node_rdd)))
        evalList:+=graphPartitionEvaluation(node_rdd)

      }while(chosenNum!=graph_1.length&&gain_max>0)//所有的点都选完或者最大增益小于0

    }//breakbale end

    println("图1的点包括：")
    graph_1.foreach(x=>print(x+" "))
    println()
    println("图2的点包括：")
    graph_2.foreach(x=>print(x+" "))
    println("最终的K-L值是="+(graphPartitionEvaluation(node_rdd)))

    println("历次的K-L变化:")
    evalList.foreach(x=>print(x+" "))
    println()

  }

}