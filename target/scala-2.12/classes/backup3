import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.SparkSession
import scala.util.control.Breaks._


class Node(var idx:Any,
           var neighbour:List[Tuple2[Any,Double]],
           var E:Double,
           var I:Double=0,
           var chosen:Boolean=false,
           var partition:Boolean=true) extends Serializable {
  var __idx:Any=idx
  var __neighbour:List[(Any, Double)]=neighbour
  var __E:Double=E
  var __I:Double=I
  var __chosen:Boolean=chosen
  var __partition:Boolean=partition//表示这个点是否在第一个图里面

  def set_E(E:Double):Node={
    this.__E = E
    return this
  }
  def set_I(I:Double):Node={
    this.__I = I
    return this
  }
  def set_chosen(chosen:Boolean):Node={
    this.__chosen = chosen
    return this
  }
  def set_partition(partition:Boolean):Node={
    this.__partition = partition
    return this
  }
  def get_idx():Any=this.__idx
  def get_neighbour():List[Any]=this.__neighbour
  def get_E():Double=this.__E
  def get_I():Double= this.__I
  def get_chosen():Boolean= this.__chosen
  def get_partition():Boolean= this.__partition

  def weight(otherNode:Node): Double ={
    val weight = this.__neighbour.filter(x=>x._1==otherNode.__idx)
    if(weight.length==0) return 0
    return weight(0)._2
  }

  def print(): Unit ={
    println(this.__idx+" E="+this.__E+" I="+this.__I
      +" partition="+this.__partition+" is chosen="+this.__chosen)
  }
}

object KernighanLin {
  def nodeUpdate(node:Node,
                 swap_node_a:Node,
                 swap_node_b:Node):Node={
    if(swap_node_a.__partition==swap_node_b.__partition){
      println("输入有错误")
      return node
    }

    val is_node_a = node.__idx==swap_node_a.__idx
    val is_node_b = node.__idx==swap_node_b.__idx
    val in_a_graph = node.__partition==swap_node_a.__partition

    val E_a = swap_node_a.__E
    val I_a = swap_node_a.__I
    val E_b = swap_node_b.__E
    val I_b = swap_node_b.__I

    val weight_ab = swap_node_a.weight(swap_node_b)
    if(is_node_a)
      return node.set_E(I_a+weight_ab).set_I(E_a-weight_ab).
        set_chosen(true).set_partition(!swap_node_a.__partition)
    if(is_node_b)
      return node.set_E(I_b+weight_ab).set_I(E_b-weight_ab).
        set_chosen(true).set_partition(!swap_node_b.__partition)

    val weight_a = node.weight(swap_node_a)
    val weight_b = node.weight(swap_node_b)

    if(weight_a==0.0&&weight_b==0.0) return node

    if(in_a_graph)
      return node.set_I(
        node.__I - weight_a + weight_b
      ).set_E(
        //这些点的E增加了与a连接的权重
        node.__E + weight_a - weight_b
      )
    else
      return node.set_I(
        node.__I + weight_a - weight_b
      ).set_E(
        //这些点的E增加了与a连接的权重
        node.__E - weight_a + weight_b
      )


  }
  def graphUpdate(nodeRDD:RDD[Node],
                   swap_node_a:Node,
                   swap_node_b:Node):RDD[Node]={

    //入口参数检查
    if((swap_node_a==null)||(swap_node_b==null)||
      (swap_node_a.__partition==swap_node_b.__partition)||
      (swap_node_a.__chosen==true)||(swap_node_b.__chosen==true))
      println("输入交换的点不合法")

    val nodeRDD_update = nodeRDD.map(
      x=>nodeUpdate(x, swap_node_a, swap_node_b)
    )

    return nodeRDD_update
  }

  def main(args: Array[String]): Unit = {

    val sparkSession = SparkSession
      .builder()
      .appName("Spark SQL basic example")
      .master("local")
      .config("spark.some.config.option", "some-value")
      .getOrCreate()
    // For implicit conversions like converting RDDs to DataFrames
    import sparkSession.implicits._
    val df = sparkSession.read.csv("test.csv")

    val partition_1  = Array[Any]("1", "2")
    val partition_2  = Array[Any]("3", "4")

    /**
      * 1、首先组织一下数据，原来是两两连接的点，现在是(点:key, tuple(点，weight):value)
      *    也就是把连接点和连接的权重组织成一个tuple
      * 2、聚合相同的点，得到邻近点的集合，注意，临近点是一个tuple(点，weight)
      *    并且转化为List
      * 3、计算每个点跟两个子图连接的权重，定义了一个函数计算
      *    思路是filter出那些在子图里面的点的集合，map一下只保留权重，进而sum
      * 4、可以通过map，生成每一个点的类，partition表示这个点在第一个graph与否
      * */


    var df_rdd = df.rdd.map(x=>(x(0), x(1)))
    df_rdd = df_rdd.union(df_rdd.map(x=>(x._2, x._1)))
    val data = df_rdd.map(
      x=>(x._1, (x._2, 1.0)
      ))
    data.foreach(println)


    var node_rdd = data.combineByKey(
      (x:(Any, Double))=>
        (
          List(x),
          //计算当前连接点跟graph1的连接权重
          if(partition_1.contains(x._1)) x._2 else 0.0,
          //计算当前连接点跟graph2的连接权重
          if(partition_2.contains(x._1)) x._2 else 0.0
        ),//相当于对value做了一次变换，方便计算

      (combineValue:(List[(Any,Double)], Double, Double), x:(Any, Double))=>
        (
          combineValue._1:+x,
          //合并：累计跟graph1的连接权重+同一个分区不同key对连接点跟graph1的权重
          combineValue._2+(if(partition_1.contains(x._1)) x._2 else 0.0),
          //合并：累计跟graph2的连接权重+同一个分区不同key对连接点跟graph2的权重
          combineValue._3+(if(partition_2.contains(x._1)) x._2 else 0.0)
        ),//聚合一个分区内部不同key的值，value对应领域
      //(if(partition_1.contains(x._1)) {println("haha");x._2} else 0.0
      (combineValue1:(List[(Any,Double)], Double, Double),
       combineValue2:(List[(Any,Double)], Double, Double))=>
        (
          combineValue1._1:::combineValue2._1,
          //合并不同分区的累计权重
          combineValue1._2+combineValue2._2,
          combineValue1._3+combineValue2._3
        )
    ).map(
      x=>(
        x._1, x._2._1,x._2._2,x._2._3,partition_1.contains(x._1)
      )
    ).map(
      x=>(
        x._1, x._2,
        if(x._5) x._4 else x._3,//在第一个图中，E是图2的连接，I是图1的连接
        if(x._5) x._3 else x._4,//在第一个图中，I是图2的连接，E是图1的连接
        x._5,
        )
    ).map(
      x=>new Node(x._1, x._2, x._3, x._4, false, x._5)
    )

    node_rdd.foreach(x=>println(x.__idx, x.__E, x.__I,
      x.__neighbour, x.__partition, x.__chosen))



    //calculate example
    var gain_max = 0.0
    var swap_node_a:Node=null
    var swap_node_b:Node=null
    var chosenNum = 0

    var count:Int = 0
    var graph1:Array[Any] = null
    var graph2:Array[Any] = null

    breakable{
      do{

        println("开始第%d轮次"(count))
//        val node_rdd_ = node_rdd
        node_rdd.foreach(x=>println(x.__idx, x.__E, x.__I,
          x.__neighbour, x.__partition, x.__chosen))


        swap_node_a = null
        swap_node_b = null
        gain_max = 0.0

        val result = node_rdd.cartesian(node_rdd).filter(
          x=>x._1.__idx.toString()<x._2.__idx.toString()&&
            x._1.__chosen!=true&&x._2.__chosen!=true&&
            x._1.partition!=x._2.partition).map(x=>{
          (x, x._1.__E-x._1.__I+x._2.__E-x._2.__I-x._1.weight(x._2))
        }).reduce((x,y)=>{
          if(x._2>=y._2) x else y
        })
        if(result._2>0){
          println("positive gain!!! find "+result._1._1.__idx+" and "+result._1._2.__idx+" with gain "+result._2)
          swap_node_a = result._1._1
          swap_node_b = result._1._2
          gain_max = result._2
        }
        else{
          println("game over!!")
          println("negative gain!!! find "+result._1._1.__idx+" and "+result._1._2.__idx+" with gain "+result._2)
        }

        println("=========我看这个扑街变了没=================")
//        node_rdd.foreach(x=>println(x.__idx, x.__E, x.__I,
//          x.__neighbour, x.__partition, x.__chosen))
        node_rdd.foreach(x=>x.print())


        if(swap_node_a==null || swap_node_b==null) {
          println("没有可以交换的点，可以离开了")
          break()
        }
        //swap
        println("swap two node : "+swap_node_a.__idx+"and"+swap_node_b.__idx+" wirh gain "+gain_max)
        chosenNum+=1

        node_rdd = graphUpdate(node_rdd, swap_node_a, swap_node_b)

        println("第%d轮结束"(count))
        count+=1
        node_rdd.foreach(x=>x.print())


        graph1 = node_rdd.filter(x=>x.__partition==true).map(x=>x.get_idx()).collect()
        graph2 = node_rdd.filter(x=>x.__partition==false).map(x=>x.get_idx()).collect()

        println("图1的点包括：")
        graph1.foreach(x=>print(x+" "))
        println("==================")
        println("图2的点包括：")
        graph2.foreach(x=>print(x+" "))

      }while(chosenNum!=partition_1.length&&gain_max>0)//所有的点都选完或者最大增益小于0

    }//breakbale end

    println("图1的点包括：")
    graph1.foreach(x=>print(x+" "))
    println()
    println("图2的点包括：")
    graph2.foreach(x=>print(x+" "))

  }

}
